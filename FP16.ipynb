{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FP16.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMwyDzeKpbT04USFYXh0n2n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lFmjHT4tVTvq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"451a134f-e786-4f53-9b87-745c1aff888f","executionInfo":{"status":"error","timestamp":1586700716603,"user_tz":-330,"elapsed":9256542,"user":{"displayName":"NEMISH MURAWAT","photoUrl":"","userId":"05040625308768212141"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import torchvision\n","import os\n","#import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","import numpy as np\n","#import torch.utils.data.distributed\n","##import torchvision.models as models\n","#import torch.nn.parallel\n","#import torch.backends.cudnn as cudnn\n","#import torch.distributed as dist\n","from torch.optim.lr_scheduler import LambdaLR\n","#parameters\n","epochs=20\n","bs=128\n","lr=0.1\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","#Create Model\n","def get_model():\n"," model=torchvision.models.resnet50(pretrained=False)\n"," return model,optim.SGD(model.parameters(),lr=lr)\n","model,opt=get_model()\n","model.half()\n","for layer in model.modules():\n","  if isinstance(layer,nn.BatchNorm2d):\n","    layer.float()\n","model.cuda()\n","\n","#lambda\n","def x(epoch):\n","  if epoch <=5:\n","     return epoch/5\n","  else:\n","     return 1\n","lambda1 = lambda epoch: x(epoch)\n","scheduler = LambdaLR(opt, [lambda1])\n","\n","\n","\n","#Loss function\n","loss_func=F.cross_entropy\n","def loss_batch(model, loss_func, xb, yb, opt=None):\n","    loss = loss_func(model(xb), yb)\n","    print(loss)\n","    if opt is not None:\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","    return loss.item(), len(xb)\n","\n","   \n","#DATA LOADER  \n","\n","train_dir=os.path.join('/content/gdrive/My Drive/imagenette2/','train')\n","val_dir=os.path.join('/content/gdrive/My Drive/imagenette2/','val')       \n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","train_dataset = datasets.ImageFolder(train_dir,transforms.Compose([transforms.RandomResizedCrop(224),\n","transforms.RandomHorizontalFlip(),transforms.ToTensor(),normalize]))\n","train_loader=DataLoader(train_dataset,batch_size=bs,shuffle=True,pin_memory=True)\n","val_loader=DataLoader(datasets.ImageFolder(val_dir,transforms.Compose([transforms.Resize(256),\n","transforms.CenterCrop(224),transforms.ToTensor(),normalize])),batch_size=bs,shuffle=False,pin_memory=True)\n","\n","\n","for epoch in range(epochs):\n","        model.train()\n","        for xb, yb in train_loader:\n","            xb=xb.to('cuda').half()\n","            yb=yb.to('cuda')\n","            loss_batch(model, loss_func, xb, yb, opt)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            \n","            losses, nums = zip(\n","                *[loss_batch(model, loss_func, xb.to('cuda').half(), yb.to('cuda')) for xb, yb in val_loader]\n","            )\n","        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n","        \n","        print(epoch, val_loss) \n","        scheduler.step()\n","        print(scheduler.get_last_lr())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","tensor(7.1367, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0742, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0625, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0703, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0859, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1914, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0117, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1211, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.2383, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0742, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1953, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(6.9570, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1523, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1875, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0469, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0547, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1289, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1211, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1133, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0938, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0625, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1289, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0430, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0430, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.2031, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0312, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1094, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0234, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1758, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0156, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0156, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1875, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1914, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0508, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.2344, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0312, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0664, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1836, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1445, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1250, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1211, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1094, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1367, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.2305, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(6.9766, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0703, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1094, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1797, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0586, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1680, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0352, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1055, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0977, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1289, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0352, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1523, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1914, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1172, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0859, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1289, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(6.9961, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1719, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1875, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.2344, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1562, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0742, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1250, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.2188, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1133, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0664, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.0195, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.1641, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(6.2148, device='cuda:0', dtype=torch.float16)\n","tensor(6.2031, device='cuda:0', dtype=torch.float16)\n","tensor(6.2148, device='cuda:0', dtype=torch.float16)\n","tensor(6.8086, device='cuda:0', dtype=torch.float16)\n","tensor(6.8320, device='cuda:0', dtype=torch.float16)\n","tensor(6.8242, device='cuda:0', dtype=torch.float16)\n","tensor(7.6094, device='cuda:0', dtype=torch.float16)\n","tensor(7.7305, device='cuda:0', dtype=torch.float16)\n","tensor(7.6562, device='cuda:0', dtype=torch.float16)\n","tensor(7.1992, device='cuda:0', dtype=torch.float16)\n","tensor(7.2305, device='cuda:0', dtype=torch.float16)\n","tensor(7.1875, device='cuda:0', dtype=torch.float16)\n","tensor(7.1016, device='cuda:0', dtype=torch.float16)\n","tensor(7.0820, device='cuda:0', dtype=torch.float16)\n","tensor(7.0742, device='cuda:0', dtype=torch.float16)\n","tensor(7.8086, device='cuda:0', dtype=torch.float16)\n","tensor(7.7500, device='cuda:0', dtype=torch.float16)\n","tensor(7.8594, device='cuda:0', dtype=torch.float16)\n","tensor(6.2461, device='cuda:0', dtype=torch.float16)\n","tensor(5.8320, device='cuda:0', dtype=torch.float16)\n","tensor(5.8242, device='cuda:0', dtype=torch.float16)\n","tensor(6.6836, device='cuda:0', dtype=torch.float16)\n","tensor(6.9492, device='cuda:0', dtype=torch.float16)\n","tensor(6.9492, device='cuda:0', dtype=torch.float16)\n","tensor(7.7617, device='cuda:0', dtype=torch.float16)\n","tensor(8.3438, device='cuda:0', dtype=torch.float16)\n","tensor(8.3672, device='cuda:0', dtype=torch.float16)\n","tensor(8.0391, device='cuda:0', dtype=torch.float16)\n","tensor(7.1914, device='cuda:0', dtype=torch.float16)\n","tensor(7.2266, device='cuda:0', dtype=torch.float16)\n","tensor(7.2578, device='cuda:0', dtype=torch.float16)\n","0 7.12953224522293\n","[0.020000000000000004]\n","tensor(7.1250, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(4.0820, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.5566, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.6035, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(4.1797, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(6.9297, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(5.8828, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(8.5391, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(7.3008, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(5.0547, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(4.5547, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(5.0781, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.9180, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(4.5547, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.8848, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.3711, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.6973, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9512, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6934, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8652, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9141, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.1152, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.6582, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.2441, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8574, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.1094, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8047, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7441, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.5098, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8594, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6836, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8164, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.2109, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9199, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8008, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9648, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.3984, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9258, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.1914, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.0215, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(4.5508, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9668, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5215, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7676, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3., device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6387, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7676, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6191, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9805, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.2617, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.4219, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.2617, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9492, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.1836, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9414, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.3789, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9609, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3965, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4219, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4570, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3887, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5176, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.1543, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9297, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.2891, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6035, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4414, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7793, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5605, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6719, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7129, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.4414, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7520, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6016, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3457, device='cuda:0', dtype=torch.float16)\n","tensor(2.3008, device='cuda:0', dtype=torch.float16)\n","tensor(2.3477, device='cuda:0', dtype=torch.float16)\n","tensor(2.0371, device='cuda:0', dtype=torch.float16)\n","tensor(2.0273, device='cuda:0', dtype=torch.float16)\n","tensor(2.0332, device='cuda:0', dtype=torch.float16)\n","tensor(2.8750, device='cuda:0', dtype=torch.float16)\n","tensor(2.9941, device='cuda:0', dtype=torch.float16)\n","tensor(2.8750, device='cuda:0', dtype=torch.float16)\n","tensor(1.1709, device='cuda:0', dtype=torch.float16)\n","tensor(1.1484, device='cuda:0', dtype=torch.float16)\n","tensor(1.4238, device='cuda:0', dtype=torch.float16)\n","tensor(4.6875, device='cuda:0', dtype=torch.float16)\n","tensor(3.9590, device='cuda:0', dtype=torch.float16)\n","tensor(3.9941, device='cuda:0', dtype=torch.float16)\n","tensor(2.6758, device='cuda:0', dtype=torch.float16)\n","tensor(2.7227, device='cuda:0', dtype=torch.float16)\n","tensor(2.7344, device='cuda:0', dtype=torch.float16)\n","tensor(2.0273, device='cuda:0', dtype=torch.float16)\n","tensor(2.0410, device='cuda:0', dtype=torch.float16)\n","tensor(1.9375, device='cuda:0', dtype=torch.float16)\n","tensor(2.2910, device='cuda:0', dtype=torch.float16)\n","tensor(2.4199, device='cuda:0', dtype=torch.float16)\n","tensor(2.5234, device='cuda:0', dtype=torch.float16)\n","tensor(4.8711, device='cuda:0', dtype=torch.float16)\n","tensor(3.5488, device='cuda:0', dtype=torch.float16)\n","tensor(3.6621, device='cuda:0', dtype=torch.float16)\n","tensor(3.5820, device='cuda:0', dtype=torch.float16)\n","tensor(3.9082, device='cuda:0', dtype=torch.float16)\n","tensor(4.0977, device='cuda:0', dtype=torch.float16)\n","tensor(3.7285, device='cuda:0', dtype=torch.float16)\n","1 2.796063395700637\n","[0.04000000000000001]\n","tensor(2.9824, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9805, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.3984, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(4.5977, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.6289, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(4.5039, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.9082, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(5.0234, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(5.0352, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.3594, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(4.6055, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(4.3047, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3., device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.0469, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.4453, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.6250, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.0664, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8223, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6797, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4199, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6719, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.2930, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7305, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7227, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7891, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.0312, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6895, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9688, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5938, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4082, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5176, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2461, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5488, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5176, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4941, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.1680, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6816, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6426, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6270, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.0234, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6973, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4688, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4492, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5625, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.0996, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6562, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.5742, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6855, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3555, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8457, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6055, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8105, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4941, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2285, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8359, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5117, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5957, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5762, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3516, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3066, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4180, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5469, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4551, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4512, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5898, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4043, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6035, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7168, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2891, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3086, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3145, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3027, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4375, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2324, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9355, device='cuda:0', dtype=torch.float16)\n","tensor(1.8477, device='cuda:0', dtype=torch.float16)\n","tensor(1.9229, device='cuda:0', dtype=torch.float16)\n","tensor(2.3848, device='cuda:0', dtype=torch.float16)\n","tensor(2.4219, device='cuda:0', dtype=torch.float16)\n","tensor(2.4180, device='cuda:0', dtype=torch.float16)\n","tensor(3.7090, device='cuda:0', dtype=torch.float16)\n","tensor(3.8652, device='cuda:0', dtype=torch.float16)\n","tensor(3.9844, device='cuda:0', dtype=torch.float16)\n","tensor(2.5605, device='cuda:0', dtype=torch.float16)\n","tensor(2.4512, device='cuda:0', dtype=torch.float16)\n","tensor(2.3809, device='cuda:0', dtype=torch.float16)\n","tensor(1.5693, device='cuda:0', dtype=torch.float16)\n","tensor(1.5771, device='cuda:0', dtype=torch.float16)\n","tensor(1.6367, device='cuda:0', dtype=torch.float16)\n","tensor(2.0137, device='cuda:0', dtype=torch.float16)\n","tensor(2.5078, device='cuda:0', dtype=torch.float16)\n","tensor(2.3184, device='cuda:0', dtype=torch.float16)\n","tensor(2.5469, device='cuda:0', dtype=torch.float16)\n","tensor(2.9336, device='cuda:0', dtype=torch.float16)\n","tensor(2.7051, device='cuda:0', dtype=torch.float16)\n","tensor(2.0586, device='cuda:0', dtype=torch.float16)\n","tensor(1.8008, device='cuda:0', dtype=torch.float16)\n","tensor(2.3359, device='cuda:0', dtype=torch.float16)\n","tensor(3.0586, device='cuda:0', dtype=torch.float16)\n","tensor(1.9434, device='cuda:0', dtype=torch.float16)\n","tensor(2.0371, device='cuda:0', dtype=torch.float16)\n","tensor(2.2656, device='cuda:0', dtype=torch.float16)\n","tensor(4.5352, device='cuda:0', dtype=torch.float16)\n","tensor(4.4414, device='cuda:0', dtype=torch.float16)\n","tensor(2.8145, device='cuda:0', dtype=torch.float16)\n","2 2.5448671377388536\n","[0.06]\n","tensor(2.3340, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4648, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4258, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7871, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6758, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6172, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5645, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2051, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7344, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6973, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8164, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5215, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2207, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3301, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4746, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5312, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4492, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4102, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5859, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2109, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4727, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5098, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2637, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4082, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4629, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6387, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3398, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3535, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5703, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6504, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8086, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5645, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.8379, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5918, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4062, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4590, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4629, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.9277, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4121, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7188, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6719, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6973, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.1895, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2559, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4512, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3379, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1777, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4609, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4082, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4512, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4688, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2324, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5430, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1562, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2207, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1641, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2754, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0820, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2949, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1406, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0820, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0977, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0820, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0859, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2871, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2500, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3184, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5820, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4531, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2988, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1719, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3223, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2637, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1152, device='cuda:0', dtype=torch.float16)\n","tensor(1.9326, device='cuda:0', dtype=torch.float16)\n","tensor(1.9395, device='cuda:0', dtype=torch.float16)\n","tensor(1.9131, device='cuda:0', dtype=torch.float16)\n","tensor(1.9404, device='cuda:0', dtype=torch.float16)\n","tensor(1.9219, device='cuda:0', dtype=torch.float16)\n","tensor(2.4805, device='cuda:0', dtype=torch.float16)\n","tensor(2.3047, device='cuda:0', dtype=torch.float16)\n","tensor(2.7305, device='cuda:0', dtype=torch.float16)\n","tensor(2.5137, device='cuda:0', dtype=torch.float16)\n","tensor(2.4121, device='cuda:0', dtype=torch.float16)\n","tensor(2.3691, device='cuda:0', dtype=torch.float16)\n","tensor(1.3457, device='cuda:0', dtype=torch.float16)\n","tensor(1.3584, device='cuda:0', dtype=torch.float16)\n","tensor(1.4092, device='cuda:0', dtype=torch.float16)\n","tensor(3.3594, device='cuda:0', dtype=torch.float16)\n","tensor(4.9219, device='cuda:0', dtype=torch.float16)\n","tensor(4.8789, device='cuda:0', dtype=torch.float16)\n","tensor(2.1074, device='cuda:0', dtype=torch.float16)\n","tensor(2.3203, device='cuda:0', dtype=torch.float16)\n","tensor(1.8662, device='cuda:0', dtype=torch.float16)\n","tensor(2.3691, device='cuda:0', dtype=torch.float16)\n","tensor(2.9766, device='cuda:0', dtype=torch.float16)\n","tensor(3.5293, device='cuda:0', dtype=torch.float16)\n","tensor(7.8125, device='cuda:0', dtype=torch.float16)\n","tensor(1.8818, device='cuda:0', dtype=torch.float16)\n","tensor(2.0391, device='cuda:0', dtype=torch.float16)\n","tensor(1.9541, device='cuda:0', dtype=torch.float16)\n","tensor(6.4375, device='cuda:0', dtype=torch.float16)\n","tensor(5.2734, device='cuda:0', dtype=torch.float16)\n","tensor(4.7734, device='cuda:0', dtype=torch.float16)\n","3 2.856240047770701\n","[0.08000000000000002]\n","tensor(2.3223, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2402, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5996, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3008, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4336, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4805, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6719, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.1758, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.9336, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.8672, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.3066, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5020, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5918, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0293, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3027, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4844, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4238, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2227, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9619, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9756, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2773, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3105, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1504, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3242, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0176, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3242, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3848, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1914, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1738, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1094, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3477, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1602, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5684, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4316, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1289, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2168, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1875, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0078, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3184, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2090, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0508, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9111, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9609, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1543, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0977, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1797, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2852, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2., device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9951, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2012, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7900, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8096, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8545, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9590, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8838, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9707, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1152, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1035, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9775, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0938, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9619, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9570, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9951, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9932, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1016, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0977, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9355, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9121, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9180, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8359, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0625, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.5547, device='cuda:0', dtype=torch.float16)\n","tensor(3.9238, device='cuda:0', dtype=torch.float16)\n","tensor(2.6133, device='cuda:0', dtype=torch.float16)\n","tensor(2.7832, device='cuda:0', dtype=torch.float16)\n","tensor(2.8008, device='cuda:0', dtype=torch.float16)\n","tensor(2.9492, device='cuda:0', dtype=torch.float16)\n","tensor(2.2520, device='cuda:0', dtype=torch.float16)\n","tensor(1.9404, device='cuda:0', dtype=torch.float16)\n","tensor(2.0859, device='cuda:0', dtype=torch.float16)\n","tensor(2.1816, device='cuda:0', dtype=torch.float16)\n","tensor(1.7910, device='cuda:0', dtype=torch.float16)\n","tensor(1.7744, device='cuda:0', dtype=torch.float16)\n","tensor(0.8379, device='cuda:0', dtype=torch.float16)\n","tensor(0.7974, device='cuda:0', dtype=torch.float16)\n","tensor(0.7988, device='cuda:0', dtype=torch.float16)\n","tensor(1.8018, device='cuda:0', dtype=torch.float16)\n","tensor(2.2051, device='cuda:0', dtype=torch.float16)\n","tensor(2.1113, device='cuda:0', dtype=torch.float16)\n","tensor(3.3340, device='cuda:0', dtype=torch.float16)\n","tensor(4.3906, device='cuda:0', dtype=torch.float16)\n","tensor(3.5078, device='cuda:0', dtype=torch.float16)\n","tensor(2.4023, device='cuda:0', dtype=torch.float16)\n","tensor(2.2383, device='cuda:0', dtype=torch.float16)\n","tensor(2.4102, device='cuda:0', dtype=torch.float16)\n","tensor(10.4453, device='cuda:0', dtype=torch.float16)\n","tensor(2.2773, device='cuda:0', dtype=torch.float16)\n","tensor(2.8184, device='cuda:0', dtype=torch.float16)\n","tensor(9.2656, device='cuda:0', dtype=torch.float16)\n","tensor(7.3672, device='cuda:0', dtype=torch.float16)\n","tensor(12.1406, device='cuda:0', dtype=torch.float16)\n","tensor(13.6797, device='cuda:0', dtype=torch.float16)\n","4 3.5834932324840763\n","[0.1]\n","tensor(2.1387, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4414, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1133, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9854, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0527, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0176, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2285, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0156, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0605, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8389, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9150, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9287, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0195, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0723, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9150, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3145, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9092, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1016, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0605, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0293, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0039, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1445, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.6797, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.0371, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(3.3926, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.7617, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.4473, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0117, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0801, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2539, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3105, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0840, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9658, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1230, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2773, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1348, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1133, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1074, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1758, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0371, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1797, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3574, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2090, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1191, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0840, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9160, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0469, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.3242, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0938, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9092, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9150, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9414, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0449, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9727, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0527, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9814, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7920, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6807, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0586, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1465, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8682, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2480, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9307, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8193, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9736, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8555, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6553, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9727, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9043, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0566, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9668, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1133, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0566, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8203, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.1973, device='cuda:0', dtype=torch.float16)\n","tensor(2.2441, device='cuda:0', dtype=torch.float16)\n","tensor(2.1934, device='cuda:0', dtype=torch.float16)\n","tensor(1.9004, device='cuda:0', dtype=torch.float16)\n","tensor(1.8818, device='cuda:0', dtype=torch.float16)\n","tensor(1.6484, device='cuda:0', dtype=torch.float16)\n","tensor(0.8989, device='cuda:0', dtype=torch.float16)\n","tensor(0.6426, device='cuda:0', dtype=torch.float16)\n","tensor(1.0605, device='cuda:0', dtype=torch.float16)\n","tensor(1.9609, device='cuda:0', dtype=torch.float16)\n","tensor(2.5586, device='cuda:0', dtype=torch.float16)\n","tensor(2.1621, device='cuda:0', dtype=torch.float16)\n","tensor(3.1973, device='cuda:0', dtype=torch.float16)\n","tensor(2.6074, device='cuda:0', dtype=torch.float16)\n","tensor(2.6367, device='cuda:0', dtype=torch.float16)\n","tensor(1.3721, device='cuda:0', dtype=torch.float16)\n","tensor(1.6289, device='cuda:0', dtype=torch.float16)\n","tensor(1.0791, device='cuda:0', dtype=torch.float16)\n","tensor(1.8359, device='cuda:0', dtype=torch.float16)\n","tensor(2.5918, device='cuda:0', dtype=torch.float16)\n","tensor(2.0391, device='cuda:0', dtype=torch.float16)\n","tensor(3.4395, device='cuda:0', dtype=torch.float16)\n","tensor(2.6172, device='cuda:0', dtype=torch.float16)\n","tensor(2.8809, device='cuda:0', dtype=torch.float16)\n","tensor(2.4688, device='cuda:0', dtype=torch.float16)\n","tensor(2.9512, device='cuda:0', dtype=torch.float16)\n","tensor(2.9883, device='cuda:0', dtype=torch.float16)\n","tensor(1.6182, device='cuda:0', dtype=torch.float16)\n","tensor(1.5225, device='cuda:0', dtype=torch.float16)\n","tensor(1.7227, device='cuda:0', dtype=torch.float16)\n","tensor(2.1758, device='cuda:0', dtype=torch.float16)\n","5 2.0868481289808916\n","[0.1]\n","tensor(1.7002, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9365, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0645, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7520, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6650, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0332, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8105, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8193, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.2051, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7529, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9502, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8105, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6592, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6465, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8105, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7764, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9072, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0156, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8604, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9844, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6191, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6572, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9043, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0352, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7393, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8584, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7549, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6865, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6074, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7627, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0703, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8428, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7188, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7900, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8623, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6787, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7891, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5098, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5771, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5635, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8027, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8594, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6846, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7324, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6064, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6670, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7510, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6699, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8125, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7979, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9580, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6699, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7715, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6123, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7539, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7598, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6807, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5273, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4482, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5186, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8555, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7432, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7090, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8174, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7871, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9814, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8320, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7500, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8340, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9229, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(2.0391, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5830, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5537, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7100, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2324, device='cuda:0', dtype=torch.float16)\n","tensor(1.4023, device='cuda:0', dtype=torch.float16)\n","tensor(1.3428, device='cuda:0', dtype=torch.float16)\n","tensor(2.1152, device='cuda:0', dtype=torch.float16)\n","tensor(1.8584, device='cuda:0', dtype=torch.float16)\n","tensor(1.5527, device='cuda:0', dtype=torch.float16)\n","tensor(1.6367, device='cuda:0', dtype=torch.float16)\n","tensor(1.2568, device='cuda:0', dtype=torch.float16)\n","tensor(1.5166, device='cuda:0', dtype=torch.float16)\n","tensor(1.8359, device='cuda:0', dtype=torch.float16)\n","tensor(1.9395, device='cuda:0', dtype=torch.float16)\n","tensor(1.9678, device='cuda:0', dtype=torch.float16)\n","tensor(1.1846, device='cuda:0', dtype=torch.float16)\n","tensor(1.0146, device='cuda:0', dtype=torch.float16)\n","tensor(1.0947, device='cuda:0', dtype=torch.float16)\n","tensor(1.4668, device='cuda:0', dtype=torch.float16)\n","tensor(1.6895, device='cuda:0', dtype=torch.float16)\n","tensor(1.3535, device='cuda:0', dtype=torch.float16)\n","tensor(2.0488, device='cuda:0', dtype=torch.float16)\n","tensor(1.8135, device='cuda:0', dtype=torch.float16)\n","tensor(2.1914, device='cuda:0', dtype=torch.float16)\n","tensor(2.2129, device='cuda:0', dtype=torch.float16)\n","tensor(2.6191, device='cuda:0', dtype=torch.float16)\n","tensor(2.0137, device='cuda:0', dtype=torch.float16)\n","tensor(2.1582, device='cuda:0', dtype=torch.float16)\n","tensor(1.6631, device='cuda:0', dtype=torch.float16)\n","tensor(2.3184, device='cuda:0', dtype=torch.float16)\n","tensor(1.7197, device='cuda:0', dtype=torch.float16)\n","tensor(1.6016, device='cuda:0', dtype=torch.float16)\n","tensor(1.6875, device='cuda:0', dtype=torch.float16)\n","tensor(1.9912, device='cuda:0', dtype=torch.float16)\n","6 1.722898835589172\n","[0.1]\n","tensor(1.4766, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8096, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8516, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.9248, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6436, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5879, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8047, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6230, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7207, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8545, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8623, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4990, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3994, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5361, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7441, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7812, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7402, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6748, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8662, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6064, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5225, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6484, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5713, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5361, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6074, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5537, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6104, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4893, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7080, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7012, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5488, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3877, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3604, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6045, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6797, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7402, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6367, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5537, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7930, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7334, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7061, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5771, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7529, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7861, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6641, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7256, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6943, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6240, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3467, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6211, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7598, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5527, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5576, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6348, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4717, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6543, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4404, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5986, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6426, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6514, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8682, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5654, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7100, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5635, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4502, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5342, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5996, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4941, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5898, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5547, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4102, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8496, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5127, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7266, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9927, device='cuda:0', dtype=torch.float16)\n","tensor(1.1816, device='cuda:0', dtype=torch.float16)\n","tensor(0.9878, device='cuda:0', dtype=torch.float16)\n","tensor(0.8599, device='cuda:0', dtype=torch.float16)\n","tensor(0.8262, device='cuda:0', dtype=torch.float16)\n","tensor(0.7485, device='cuda:0', dtype=torch.float16)\n","tensor(1.0312, device='cuda:0', dtype=torch.float16)\n","tensor(1.0420, device='cuda:0', dtype=torch.float16)\n","tensor(1.1064, device='cuda:0', dtype=torch.float16)\n","tensor(1.6211, device='cuda:0', dtype=torch.float16)\n","tensor(1.5947, device='cuda:0', dtype=torch.float16)\n","tensor(1.6631, device='cuda:0', dtype=torch.float16)\n","tensor(2.4785, device='cuda:0', dtype=torch.float16)\n","tensor(2.3750, device='cuda:0', dtype=torch.float16)\n","tensor(2.4434, device='cuda:0', dtype=torch.float16)\n","tensor(2.8047, device='cuda:0', dtype=torch.float16)\n","tensor(3.3730, device='cuda:0', dtype=torch.float16)\n","tensor(2.7988, device='cuda:0', dtype=torch.float16)\n","tensor(1.4141, device='cuda:0', dtype=torch.float16)\n","tensor(0.9639, device='cuda:0', dtype=torch.float16)\n","tensor(1.1797, device='cuda:0', dtype=torch.float16)\n","tensor(1.8203, device='cuda:0', dtype=torch.float16)\n","tensor(2.0078, device='cuda:0', dtype=torch.float16)\n","tensor(2.0137, device='cuda:0', dtype=torch.float16)\n","tensor(1.7695, device='cuda:0', dtype=torch.float16)\n","tensor(1.5508, device='cuda:0', dtype=torch.float16)\n","tensor(1.5859, device='cuda:0', dtype=torch.float16)\n","tensor(1.3857, device='cuda:0', dtype=torch.float16)\n","tensor(0.9053, device='cuda:0', dtype=torch.float16)\n","tensor(0.9897, device='cuda:0', dtype=torch.float16)\n","tensor(1.2725, device='cuda:0', dtype=torch.float16)\n","7 1.5770946954617835\n","[0.1]\n","tensor(1.4180, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2197, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5078, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4492, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5840, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8516, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6777, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6455, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5381, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7158, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6426, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5967, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6172, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2676, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5029, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4082, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7070, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4570, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4658, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6562, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5352, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6172, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6680, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5889, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4961, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3633, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3828, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6182, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8799, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3857, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3398, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6309, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3389, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6436, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3691, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5215, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3799, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4902, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.7080, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4258, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4688, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5186, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6445, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3809, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2842, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5010, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4180, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4521, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6104, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4424, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5127, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4707, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4814, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2402, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3828, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4551, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3857, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4971, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4043, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4531, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6943, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5107, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3467, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4277, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5088, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4531, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3838, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5273, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3916, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4922, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6045, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4180, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4463, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5596, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.8457, device='cuda:0', dtype=torch.float16)\n","tensor(2.1133, device='cuda:0', dtype=torch.float16)\n","tensor(1.8857, device='cuda:0', dtype=torch.float16)\n","tensor(2.0215, device='cuda:0', dtype=torch.float16)\n","tensor(1.9365, device='cuda:0', dtype=torch.float16)\n","tensor(2.0059, device='cuda:0', dtype=torch.float16)\n","tensor(1.6504, device='cuda:0', dtype=torch.float16)\n","tensor(1.3633, device='cuda:0', dtype=torch.float16)\n","tensor(1.7559, device='cuda:0', dtype=torch.float16)\n","tensor(2.0371, device='cuda:0', dtype=torch.float16)\n","tensor(1.8838, device='cuda:0', dtype=torch.float16)\n","tensor(1.8037, device='cuda:0', dtype=torch.float16)\n","tensor(1.6123, device='cuda:0', dtype=torch.float16)\n","tensor(1.6895, device='cuda:0', dtype=torch.float16)\n","tensor(1.7334, device='cuda:0', dtype=torch.float16)\n","tensor(1.3281, device='cuda:0', dtype=torch.float16)\n","tensor(1.5195, device='cuda:0', dtype=torch.float16)\n","tensor(1.0811, device='cuda:0', dtype=torch.float16)\n","tensor(0.6074, device='cuda:0', dtype=torch.float16)\n","tensor(0.5122, device='cuda:0', dtype=torch.float16)\n","tensor(0.6045, device='cuda:0', dtype=torch.float16)\n","tensor(1.1514, device='cuda:0', dtype=torch.float16)\n","tensor(1.2842, device='cuda:0', dtype=torch.float16)\n","tensor(1.5303, device='cuda:0', dtype=torch.float16)\n","tensor(1.7139, device='cuda:0', dtype=torch.float16)\n","tensor(2.1191, device='cuda:0', dtype=torch.float16)\n","tensor(1.7139, device='cuda:0', dtype=torch.float16)\n","tensor(2.1641, device='cuda:0', dtype=torch.float16)\n","tensor(3.0645, device='cuda:0', dtype=torch.float16)\n","tensor(2.9141, device='cuda:0', dtype=torch.float16)\n","tensor(3.1621, device='cuda:0', dtype=torch.float16)\n","8 1.7201189291401273\n","[0.1]\n","tensor(1.2852, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2236, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4521, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2871, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3535, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3213, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4492, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3818, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5625, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5020, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5635, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2783, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2773, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3975, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3125, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2939, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3037, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3984, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5439, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3867, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4756, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2529, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5000, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5674, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3955, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4336, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3027, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2344, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5908, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2266, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2959, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4268, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3398, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3857, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4346, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4072, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4629, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3877, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4619, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2773, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4678, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4287, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3564, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4062, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3838, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5039, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5088, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5625, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3613, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2949, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2461, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6992, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3135, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3945, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2842, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3848, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2852, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2529, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2441, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3018, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3145, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3066, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3330, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2432, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2617, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4863, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2412, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4092, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4717, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5059, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4082, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4736, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3867, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4531, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2705, device='cuda:0', dtype=torch.float16)\n","tensor(1.6426, device='cuda:0', dtype=torch.float16)\n","tensor(1.2910, device='cuda:0', dtype=torch.float16)\n","tensor(0.8188, device='cuda:0', dtype=torch.float16)\n","tensor(0.8296, device='cuda:0', dtype=torch.float16)\n","tensor(0.7075, device='cuda:0', dtype=torch.float16)\n","tensor(1.5703, device='cuda:0', dtype=torch.float16)\n","tensor(1.5293, device='cuda:0', dtype=torch.float16)\n","tensor(1.8379, device='cuda:0', dtype=torch.float16)\n","tensor(1.5059, device='cuda:0', dtype=torch.float16)\n","tensor(1.3975, device='cuda:0', dtype=torch.float16)\n","tensor(1.4717, device='cuda:0', dtype=torch.float16)\n","tensor(1.9189, device='cuda:0', dtype=torch.float16)\n","tensor(1.7842, device='cuda:0', dtype=torch.float16)\n","tensor(1.9805, device='cuda:0', dtype=torch.float16)\n","tensor(0.8408, device='cuda:0', dtype=torch.float16)\n","tensor(0.7793, device='cuda:0', dtype=torch.float16)\n","tensor(0.5703, device='cuda:0', dtype=torch.float16)\n","tensor(0.6372, device='cuda:0', dtype=torch.float16)\n","tensor(0.4648, device='cuda:0', dtype=torch.float16)\n","tensor(0.7368, device='cuda:0', dtype=torch.float16)\n","tensor(1.6621, device='cuda:0', dtype=torch.float16)\n","tensor(1.9336, device='cuda:0', dtype=torch.float16)\n","tensor(2.1523, device='cuda:0', dtype=torch.float16)\n","tensor(1.8613, device='cuda:0', dtype=torch.float16)\n","tensor(1.6094, device='cuda:0', dtype=torch.float16)\n","tensor(1.5488, device='cuda:0', dtype=torch.float16)\n","tensor(1.3174, device='cuda:0', dtype=torch.float16)\n","tensor(0.9683, device='cuda:0', dtype=torch.float16)\n","tensor(1.0010, device='cuda:0', dtype=torch.float16)\n","tensor(1.3838, device='cuda:0', dtype=torch.float16)\n","9 1.3226744128184713\n","[0.1]\n","tensor(1.2568, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3945, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3604, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1650, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4238, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3047, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3643, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2588, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3086, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3369, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2607, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2236, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4141, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2246, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2891, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3506, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9292, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3438, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2910, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3271, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0781, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3906, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2314, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3008, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2402, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3398, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2441, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4473, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3369, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2910, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2422, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4619, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3857, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2471, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2559, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1631, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2441, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3779, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3457, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2920, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2646, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1631, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1172, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4531, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1797, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1357, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3057, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3086, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4150, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3076, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2148, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2559, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2822, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3115, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4072, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4170, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4785, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3379, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2559, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3350, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2178, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4502, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2871, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3682, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4443, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1592, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4053, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2627, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0488, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2041, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1953, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2500, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4209, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2412, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.4990, device='cuda:0', dtype=torch.float16)\n","tensor(0.7861, device='cuda:0', dtype=torch.float16)\n","tensor(0.4375, device='cuda:0', dtype=torch.float16)\n","tensor(1.1973, device='cuda:0', dtype=torch.float16)\n","tensor(1.0703, device='cuda:0', dtype=torch.float16)\n","tensor(1.0264, device='cuda:0', dtype=torch.float16)\n","tensor(1.1191, device='cuda:0', dtype=torch.float16)\n","tensor(0.9985, device='cuda:0', dtype=torch.float16)\n","tensor(1.1553, device='cuda:0', dtype=torch.float16)\n","tensor(2.1250, device='cuda:0', dtype=torch.float16)\n","tensor(2.0645, device='cuda:0', dtype=torch.float16)\n","tensor(1.9102, device='cuda:0', dtype=torch.float16)\n","tensor(1.6230, device='cuda:0', dtype=torch.float16)\n","tensor(1.4346, device='cuda:0', dtype=torch.float16)\n","tensor(1.5488, device='cuda:0', dtype=torch.float16)\n","tensor(0.7798, device='cuda:0', dtype=torch.float16)\n","tensor(0.7983, device='cuda:0', dtype=torch.float16)\n","tensor(0.5581, device='cuda:0', dtype=torch.float16)\n","tensor(1.2119, device='cuda:0', dtype=torch.float16)\n","tensor(1.1660, device='cuda:0', dtype=torch.float16)\n","tensor(1.5059, device='cuda:0', dtype=torch.float16)\n","tensor(1.2539, device='cuda:0', dtype=torch.float16)\n","tensor(1.2686, device='cuda:0', dtype=torch.float16)\n","tensor(1.3467, device='cuda:0', dtype=torch.float16)\n","tensor(1.4609, device='cuda:0', dtype=torch.float16)\n","tensor(1.2529, device='cuda:0', dtype=torch.float16)\n","tensor(1.5742, device='cuda:0', dtype=torch.float16)\n","tensor(1.2334, device='cuda:0', dtype=torch.float16)\n","tensor(1.3506, device='cuda:0', dtype=torch.float16)\n","tensor(1.3662, device='cuda:0', dtype=torch.float16)\n","tensor(1.8262, device='cuda:0', dtype=torch.float16)\n","10 1.250184613853503\n","[0.1]\n","tensor(1.2988, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0801, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1455, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1660, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1885, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2656, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3281, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0547, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2080, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2285, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2959, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3291, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1172, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0938, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3926, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1279, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1377, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2314, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2344, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1455, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2715, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3115, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2871, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0449, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2012, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1846, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0850, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2188, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2061, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1611, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3828, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1953, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2500, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3721, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3262, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1738, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3994, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3057, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2783, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1152, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1475, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2051, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2949, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3760, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2949, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2578, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3574, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0449, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3066, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4277, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2236, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0381, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0645, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2656, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1885, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1670, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4307, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3740, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3242, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1426, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3164, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4307, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1240, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0068, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9766, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9878, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2715, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2012, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0703, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9976, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2969, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5586, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4775, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2197, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3896, device='cuda:0', dtype=torch.float16)\n","tensor(1.6465, device='cuda:0', dtype=torch.float16)\n","tensor(1.4697, device='cuda:0', dtype=torch.float16)\n","tensor(0.7295, device='cuda:0', dtype=torch.float16)\n","tensor(0.5991, device='cuda:0', dtype=torch.float16)\n","tensor(0.6074, device='cuda:0', dtype=torch.float16)\n","tensor(0.9834, device='cuda:0', dtype=torch.float16)\n","tensor(0.7520, device='cuda:0', dtype=torch.float16)\n","tensor(1.1240, device='cuda:0', dtype=torch.float16)\n","tensor(1.9814, device='cuda:0', dtype=torch.float16)\n","tensor(1.8818, device='cuda:0', dtype=torch.float16)\n","tensor(1.7656, device='cuda:0', dtype=torch.float16)\n","tensor(1.0527, device='cuda:0', dtype=torch.float16)\n","tensor(1.0449, device='cuda:0', dtype=torch.float16)\n","tensor(1.0605, device='cuda:0', dtype=torch.float16)\n","tensor(0.8208, device='cuda:0', dtype=torch.float16)\n","tensor(0.8535, device='cuda:0', dtype=torch.float16)\n","tensor(0.6958, device='cuda:0', dtype=torch.float16)\n","tensor(0.9927, device='cuda:0', dtype=torch.float16)\n","tensor(1.0176, device='cuda:0', dtype=torch.float16)\n","tensor(1.3047, device='cuda:0', dtype=torch.float16)\n","tensor(1.0957, device='cuda:0', dtype=torch.float16)\n","tensor(1.0791, device='cuda:0', dtype=torch.float16)\n","tensor(1.1689, device='cuda:0', dtype=torch.float16)\n","tensor(1.5508, device='cuda:0', dtype=torch.float16)\n","tensor(2.2090, device='cuda:0', dtype=torch.float16)\n","tensor(1.8643, device='cuda:0', dtype=torch.float16)\n","tensor(1.6143, device='cuda:0', dtype=torch.float16)\n","tensor(1.2432, device='cuda:0', dtype=torch.float16)\n","tensor(1.2168, device='cuda:0', dtype=torch.float16)\n","tensor(1.7041, device='cuda:0', dtype=torch.float16)\n","11 1.237509205812102\n","[0.1]\n","tensor(1.2432, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0986, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1113, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0342, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1172, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2100, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2139, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2207, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1572, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1836, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1299, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2979, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2910, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1855, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1260, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2646, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2607, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5156, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2988, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0010, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2725, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2217, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1670, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1416, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9751, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1943, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1318, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0645, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3486, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1035, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2158, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2852, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0918, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1367, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3623, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2109, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1836, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0996, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2334, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6025, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1533, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2969, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0459, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1074, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2617, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2490, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3975, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3906, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2568, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0078, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1621, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0898, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1338, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0654, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3555, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1602, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2188, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1465, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2988, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1787, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1133, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0664, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0254, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1943, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3203, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2119, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2979, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1123, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.6162, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0020, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4561, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9590, device='cuda:0', dtype=torch.float16)\n","tensor(1.3057, device='cuda:0', dtype=torch.float16)\n","tensor(1.3643, device='cuda:0', dtype=torch.float16)\n","tensor(1.7979, device='cuda:0', dtype=torch.float16)\n","tensor(1.8057, device='cuda:0', dtype=torch.float16)\n","tensor(1.6143, device='cuda:0', dtype=torch.float16)\n","tensor(1.1660, device='cuda:0', dtype=torch.float16)\n","tensor(1.1250, device='cuda:0', dtype=torch.float16)\n","tensor(1.5244, device='cuda:0', dtype=torch.float16)\n","tensor(2.9434, device='cuda:0', dtype=torch.float16)\n","tensor(3.3906, device='cuda:0', dtype=torch.float16)\n","tensor(4.2891, device='cuda:0', dtype=torch.float16)\n","tensor(1.3047, device='cuda:0', dtype=torch.float16)\n","tensor(1.0703, device='cuda:0', dtype=torch.float16)\n","tensor(1.2158, device='cuda:0', dtype=torch.float16)\n","tensor(2.3145, device='cuda:0', dtype=torch.float16)\n","tensor(2.8027, device='cuda:0', dtype=torch.float16)\n","tensor(1.6465, device='cuda:0', dtype=torch.float16)\n","tensor(1.6924, device='cuda:0', dtype=torch.float16)\n","tensor(1.3809, device='cuda:0', dtype=torch.float16)\n","tensor(1.5820, device='cuda:0', dtype=torch.float16)\n","tensor(3.4277, device='cuda:0', dtype=torch.float16)\n","tensor(3.3477, device='cuda:0', dtype=torch.float16)\n","tensor(2.5449, device='cuda:0', dtype=torch.float16)\n","tensor(1.6494, device='cuda:0', dtype=torch.float16)\n","tensor(0.5732, device='cuda:0', dtype=torch.float16)\n","tensor(0.6401, device='cuda:0', dtype=torch.float16)\n","tensor(1.2842, device='cuda:0', dtype=torch.float16)\n","tensor(1.2754, device='cuda:0', dtype=torch.float16)\n","tensor(2.1152, device='cuda:0', dtype=torch.float16)\n","tensor(6.0586, device='cuda:0', dtype=torch.float16)\n","12 1.9298198646496816\n","[0.1]\n","tensor(1.2314, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3848, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2168, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9902, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0430, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1182, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9873, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2148, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1689, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2959, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0674, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1406, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1836, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1162, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2295, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0391, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2168, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9985, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2061, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0029, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8911, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0137, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1172, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2822, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2529, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2734, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2051, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3057, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1475, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2227, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4756, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1875, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2109, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3027, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0361, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3076, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1143, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1426, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0938, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1016, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3916, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2295, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1533, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1025, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2373, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9985, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1992, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0244, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0596, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1328, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1826, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2637, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0879, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0762, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0234, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2178, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0176, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1514, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0459, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.5273, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.4346, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0811, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1133, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2139, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9907, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0332, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8662, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2285, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3008, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0576, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0488, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1426, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1348, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.4697, device='cuda:0', dtype=torch.float16)\n","tensor(0.6089, device='cuda:0', dtype=torch.float16)\n","tensor(0.3713, device='cuda:0', dtype=torch.float16)\n","tensor(0.6436, device='cuda:0', dtype=torch.float16)\n","tensor(0.4023, device='cuda:0', dtype=torch.float16)\n","tensor(0.5508, device='cuda:0', dtype=torch.float16)\n","tensor(1.2236, device='cuda:0', dtype=torch.float16)\n","tensor(0.9707, device='cuda:0', dtype=torch.float16)\n","tensor(1.3506, device='cuda:0', dtype=torch.float16)\n","tensor(2.3906, device='cuda:0', dtype=torch.float16)\n","tensor(2.3926, device='cuda:0', dtype=torch.float16)\n","tensor(2.4141, device='cuda:0', dtype=torch.float16)\n","tensor(1.3809, device='cuda:0', dtype=torch.float16)\n","tensor(1.4600, device='cuda:0', dtype=torch.float16)\n","tensor(1.3340, device='cuda:0', dtype=torch.float16)\n","tensor(1.1426, device='cuda:0', dtype=torch.float16)\n","tensor(1.1602, device='cuda:0', dtype=torch.float16)\n","tensor(0.8760, device='cuda:0', dtype=torch.float16)\n","tensor(1.6250, device='cuda:0', dtype=torch.float16)\n","tensor(1.4980, device='cuda:0', dtype=torch.float16)\n","tensor(2.3027, device='cuda:0', dtype=torch.float16)\n","tensor(1.1221, device='cuda:0', dtype=torch.float16)\n","tensor(0.9331, device='cuda:0', dtype=torch.float16)\n","tensor(0.9917, device='cuda:0', dtype=torch.float16)\n","tensor(1.1777, device='cuda:0', dtype=torch.float16)\n","tensor(1.3926, device='cuda:0', dtype=torch.float16)\n","tensor(1.2998, device='cuda:0', dtype=torch.float16)\n","tensor(1.3076, device='cuda:0', dtype=torch.float16)\n","tensor(1.0527, device='cuda:0', dtype=torch.float16)\n","tensor(1.0605, device='cuda:0', dtype=torch.float16)\n","tensor(1.4688, device='cuda:0', dtype=torch.float16)\n","13 1.2353662420382165\n","[0.1]\n","tensor(1.2158, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8223, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0332, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0430, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0215, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8433, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2158, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2285, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2422, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0625, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9185, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9048, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0879, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3223, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0859, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0801, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0605, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7988, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0996, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1787, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1592, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3584, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1592, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3135, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1836, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0908, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1006, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0254, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9883, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0664, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0713, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1777, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1904, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0732, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1211, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2812, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1104, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2393, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1748, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1250, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1992, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2070, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9019, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1914, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1191, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9590, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2119, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1338, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1758, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1094, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9790, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2881, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9966, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1064, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9927, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1289, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9238, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1582, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0352, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9121, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1631, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3271, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2129, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1426, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9521, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0225, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0020, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0283, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0811, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0908, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2871, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9263, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2119, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9932, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1123, device='cuda:0', dtype=torch.float16)\n","tensor(1.3418, device='cuda:0', dtype=torch.float16)\n","tensor(1.1279, device='cuda:0', dtype=torch.float16)\n","tensor(0.8110, device='cuda:0', dtype=torch.float16)\n","tensor(0.7676, device='cuda:0', dtype=torch.float16)\n","tensor(0.6870, device='cuda:0', dtype=torch.float16)\n","tensor(1.3027, device='cuda:0', dtype=torch.float16)\n","tensor(1.2188, device='cuda:0', dtype=torch.float16)\n","tensor(1.3613, device='cuda:0', dtype=torch.float16)\n","tensor(1.9033, device='cuda:0', dtype=torch.float16)\n","tensor(1.5635, device='cuda:0', dtype=torch.float16)\n","tensor(1.6357, device='cuda:0', dtype=torch.float16)\n","tensor(0.6499, device='cuda:0', dtype=torch.float16)\n","tensor(0.6118, device='cuda:0', dtype=torch.float16)\n","tensor(0.6133, device='cuda:0', dtype=torch.float16)\n","tensor(1.1426, device='cuda:0', dtype=torch.float16)\n","tensor(1.5342, device='cuda:0', dtype=torch.float16)\n","tensor(1.1436, device='cuda:0', dtype=torch.float16)\n","tensor(1.1826, device='cuda:0', dtype=torch.float16)\n","tensor(1.0400, device='cuda:0', dtype=torch.float16)\n","tensor(1.2188, device='cuda:0', dtype=torch.float16)\n","tensor(0.7358, device='cuda:0', dtype=torch.float16)\n","tensor(0.6416, device='cuda:0', dtype=torch.float16)\n","tensor(0.7051, device='cuda:0', dtype=torch.float16)\n","tensor(0.9868, device='cuda:0', dtype=torch.float16)\n","tensor(1.2910, device='cuda:0', dtype=torch.float16)\n","tensor(1.1055, device='cuda:0', dtype=torch.float16)\n","tensor(1.1172, device='cuda:0', dtype=torch.float16)\n","tensor(0.9380, device='cuda:0', dtype=torch.float16)\n","tensor(1.0156, device='cuda:0', dtype=torch.float16)\n","tensor(1.3857, device='cuda:0', dtype=torch.float16)\n","14 1.0900893212579619\n","[0.1]\n","tensor(0.9736, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0791, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9536, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0996, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1240, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1738, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0596, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0557, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1494, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0078, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1260, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0830, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9580, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9888, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0889, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0605, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0674, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0117, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1621, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2314, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3223, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0615, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2217, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0771, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0498, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9541, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1582, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2705, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0391, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0186, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1094, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8569, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9375, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0156, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2793, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0879, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1133, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2305, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9990, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8154, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9399, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2324, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0908, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2021, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2861, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0840, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1572, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0498, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9258, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9966, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9668, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1934, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0801, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0049, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2471, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1846, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9258, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7603, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9565, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9419, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0449, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9609, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1846, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1162, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0439, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2148, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9902, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1191, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0615, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0586, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1318, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0820, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9014, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9375, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2979, device='cuda:0', dtype=torch.float16)\n","tensor(1.6270, device='cuda:0', dtype=torch.float16)\n","tensor(1.4502, device='cuda:0', dtype=torch.float16)\n","tensor(0.2949, device='cuda:0', dtype=torch.float16)\n","tensor(0.1669, device='cuda:0', dtype=torch.float16)\n","tensor(0.1730, device='cuda:0', dtype=torch.float16)\n","tensor(0.8818, device='cuda:0', dtype=torch.float16)\n","tensor(0.7666, device='cuda:0', dtype=torch.float16)\n","tensor(0.9399, device='cuda:0', dtype=torch.float16)\n","tensor(0.9189, device='cuda:0', dtype=torch.float16)\n","tensor(0.8306, device='cuda:0', dtype=torch.float16)\n","tensor(0.9028, device='cuda:0', dtype=torch.float16)\n","tensor(0.5967, device='cuda:0', dtype=torch.float16)\n","tensor(0.6055, device='cuda:0', dtype=torch.float16)\n","tensor(0.5522, device='cuda:0', dtype=torch.float16)\n","tensor(1.3867, device='cuda:0', dtype=torch.float16)\n","tensor(1.8535, device='cuda:0', dtype=torch.float16)\n","tensor(1.2666, device='cuda:0', dtype=torch.float16)\n","tensor(0.8877, device='cuda:0', dtype=torch.float16)\n","tensor(0.6167, device='cuda:0', dtype=torch.float16)\n","tensor(0.9346, device='cuda:0', dtype=torch.float16)\n","tensor(1.2764, device='cuda:0', dtype=torch.float16)\n","tensor(1.5518, device='cuda:0', dtype=torch.float16)\n","tensor(1.5889, device='cuda:0', dtype=torch.float16)\n","tensor(1.5830, device='cuda:0', dtype=torch.float16)\n","tensor(1.6709, device='cuda:0', dtype=torch.float16)\n","tensor(1.6006, device='cuda:0', dtype=torch.float16)\n","tensor(1.2920, device='cuda:0', dtype=torch.float16)\n","tensor(0.7559, device='cuda:0', dtype=torch.float16)\n","tensor(0.7832, device='cuda:0', dtype=torch.float16)\n","tensor(1.1924, device='cuda:0', dtype=torch.float16)\n","15 1.0385133857484077\n","[0.1]\n","tensor(1.0771, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0225, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9741, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0537, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1777, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1006, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8872, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8652, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9653, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0762, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0312, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9565, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1201, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8071, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0176, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0332, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0010, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9780, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1143, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9766, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2539, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9106, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9976, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9116, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0127, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9561, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8647, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0459, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2695, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9302, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8047, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0547, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0850, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0225, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1826, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1895, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0322, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0498, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0059, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9609, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9917, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0225, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0557, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0107, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8667, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9292, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1455, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0654, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8154, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1016, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0303, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9048, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2793, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9893, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9009, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8809, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9668, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8877, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9438, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2598, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0059, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0166, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1016, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1104, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1055, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0752, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1553, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1758, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9771, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0693, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1855, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0791, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0166, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1191, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3643, device='cuda:0', dtype=torch.float16)\n","tensor(1.6641, device='cuda:0', dtype=torch.float16)\n","tensor(1.6426, device='cuda:0', dtype=torch.float16)\n","tensor(3.3125, device='cuda:0', dtype=torch.float16)\n","tensor(3.3281, device='cuda:0', dtype=torch.float16)\n","tensor(3.1113, device='cuda:0', dtype=torch.float16)\n","tensor(1.5107, device='cuda:0', dtype=torch.float16)\n","tensor(1.3086, device='cuda:0', dtype=torch.float16)\n","tensor(1.2803, device='cuda:0', dtype=torch.float16)\n","tensor(1.2393, device='cuda:0', dtype=torch.float16)\n","tensor(1.0127, device='cuda:0', dtype=torch.float16)\n","tensor(1.0703, device='cuda:0', dtype=torch.float16)\n","tensor(1.1016, device='cuda:0', dtype=torch.float16)\n","tensor(0.8618, device='cuda:0', dtype=torch.float16)\n","tensor(0.9014, device='cuda:0', dtype=torch.float16)\n","tensor(1.4053, device='cuda:0', dtype=torch.float16)\n","tensor(2.1016, device='cuda:0', dtype=torch.float16)\n","tensor(1.5176, device='cuda:0', dtype=torch.float16)\n","tensor(0.8032, device='cuda:0', dtype=torch.float16)\n","tensor(0.3459, device='cuda:0', dtype=torch.float16)\n","tensor(0.5430, device='cuda:0', dtype=torch.float16)\n","tensor(1.1074, device='cuda:0', dtype=torch.float16)\n","tensor(1.1514, device='cuda:0', dtype=torch.float16)\n","tensor(1.3369, device='cuda:0', dtype=torch.float16)\n","tensor(1.5938, device='cuda:0', dtype=torch.float16)\n","tensor(2.5879, device='cuda:0', dtype=torch.float16)\n","tensor(2.2598, device='cuda:0', dtype=torch.float16)\n","tensor(1.6104, device='cuda:0', dtype=torch.float16)\n","tensor(1.1299, device='cuda:0', dtype=torch.float16)\n","tensor(1.1846, device='cuda:0', dtype=torch.float16)\n","tensor(1.9609, device='cuda:0', dtype=torch.float16)\n","16 1.5226333598726114\n","[0.1]\n","tensor(0.9570, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9395, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1602, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2334, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0039, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9863, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2227, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2256, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8867, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1904, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0537, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9463, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9219, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0010, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8843, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9038, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8604, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8857, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0615, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7671, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0684, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8901, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.6489, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0469, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0703, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9009, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1641, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9785, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0234, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1240, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7964, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9238, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9878, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0703, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7500, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1611, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9595, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1514, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8848, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0566, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0547, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7905, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8477, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9463, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1270, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9517, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0039, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1514, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0635, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8892, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8911, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9243, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9536, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0752, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0547, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8672, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0537, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2207, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0518, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7871, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0273, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1182, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0566, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0029, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0215, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0176, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9551, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0869, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0645, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0195, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9966, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0205, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9980, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0781, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3779, device='cuda:0', dtype=torch.float16)\n","tensor(1.6406, device='cuda:0', dtype=torch.float16)\n","tensor(1.4629, device='cuda:0', dtype=torch.float16)\n","tensor(1.9707, device='cuda:0', dtype=torch.float16)\n","tensor(1.6104, device='cuda:0', dtype=torch.float16)\n","tensor(1.8438, device='cuda:0', dtype=torch.float16)\n","tensor(1.3809, device='cuda:0', dtype=torch.float16)\n","tensor(1.4248, device='cuda:0', dtype=torch.float16)\n","tensor(1.5088, device='cuda:0', dtype=torch.float16)\n","tensor(2.3008, device='cuda:0', dtype=torch.float16)\n","tensor(2.2363, device='cuda:0', dtype=torch.float16)\n","tensor(2.2500, device='cuda:0', dtype=torch.float16)\n","tensor(1.3770, device='cuda:0', dtype=torch.float16)\n","tensor(1.2256, device='cuda:0', dtype=torch.float16)\n","tensor(1.2031, device='cuda:0', dtype=torch.float16)\n","tensor(0.9849, device='cuda:0', dtype=torch.float16)\n","tensor(1.2412, device='cuda:0', dtype=torch.float16)\n","tensor(0.9824, device='cuda:0', dtype=torch.float16)\n","tensor(0.4854, device='cuda:0', dtype=torch.float16)\n","tensor(0.3040, device='cuda:0', dtype=torch.float16)\n","tensor(0.4353, device='cuda:0', dtype=torch.float16)\n","tensor(0.6157, device='cuda:0', dtype=torch.float16)\n","tensor(0.7935, device='cuda:0', dtype=torch.float16)\n","tensor(0.7808, device='cuda:0', dtype=torch.float16)\n","tensor(1.6328, device='cuda:0', dtype=torch.float16)\n","tensor(2.8594, device='cuda:0', dtype=torch.float16)\n","tensor(3.0020, device='cuda:0', dtype=torch.float16)\n","tensor(1.9443, device='cuda:0', dtype=torch.float16)\n","tensor(0.7363, device='cuda:0', dtype=torch.float16)\n","tensor(0.8647, device='cuda:0', dtype=torch.float16)\n","tensor(1.2041, device='cuda:0', dtype=torch.float16)\n","17 1.4112830911624203\n","[0.1]\n","tensor(0.9385, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0322, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9429, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0303, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9453, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9131, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9634, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1250, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0410, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9624, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9487, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1318, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0078, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0391, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0762, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0879, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0195, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8687, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9482, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0713, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9297, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9116, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8296, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9404, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0127, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7729, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8496, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8931, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7695, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9438, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9883, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9829, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8945, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9199, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8369, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.6914, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7983, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8184, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0869, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1006, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0938, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1270, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0420, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1016, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9121, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8823, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9570, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8374, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7363, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0518, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0938, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8721, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8452, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9072, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9761, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9072, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8193, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1895, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8237, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0938, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8408, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7520, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0518, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0068, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0312, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0850, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0176, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9980, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0205, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1875, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1094, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0049, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1035, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1738, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.5869, device='cuda:0', dtype=torch.float16)\n","tensor(0.7285, device='cuda:0', dtype=torch.float16)\n","tensor(0.6650, device='cuda:0', dtype=torch.float16)\n","tensor(1.2822, device='cuda:0', dtype=torch.float16)\n","tensor(1.1914, device='cuda:0', dtype=torch.float16)\n","tensor(1.2295, device='cuda:0', dtype=torch.float16)\n","tensor(0.8906, device='cuda:0', dtype=torch.float16)\n","tensor(0.6670, device='cuda:0', dtype=torch.float16)\n","tensor(0.8979, device='cuda:0', dtype=torch.float16)\n","tensor(1.4805, device='cuda:0', dtype=torch.float16)\n","tensor(1.2607, device='cuda:0', dtype=torch.float16)\n","tensor(1.3623, device='cuda:0', dtype=torch.float16)\n","tensor(1.4268, device='cuda:0', dtype=torch.float16)\n","tensor(1.2168, device='cuda:0', dtype=torch.float16)\n","tensor(1.2646, device='cuda:0', dtype=torch.float16)\n","tensor(1.0078, device='cuda:0', dtype=torch.float16)\n","tensor(1.3594, device='cuda:0', dtype=torch.float16)\n","tensor(1.0557, device='cuda:0', dtype=torch.float16)\n","tensor(0.6826, device='cuda:0', dtype=torch.float16)\n","tensor(0.5518, device='cuda:0', dtype=torch.float16)\n","tensor(0.8027, device='cuda:0', dtype=torch.float16)\n","tensor(0.4749, device='cuda:0', dtype=torch.float16)\n","tensor(0.5220, device='cuda:0', dtype=torch.float16)\n","tensor(0.5972, device='cuda:0', dtype=torch.float16)\n","tensor(1.1289, device='cuda:0', dtype=torch.float16)\n","tensor(2.0391, device='cuda:0', dtype=torch.float16)\n","tensor(1.9277, device='cuda:0', dtype=torch.float16)\n","tensor(1.3223, device='cuda:0', dtype=torch.float16)\n","tensor(0.6694, device='cuda:0', dtype=torch.float16)\n","tensor(0.8169, device='cuda:0', dtype=torch.float16)\n","tensor(1.0723, device='cuda:0', dtype=torch.float16)\n","18 1.03773537022293\n","[0.1]\n","tensor(0.9351, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9087, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8164, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0020, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9897, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8467, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8267, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9575, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9995, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0156, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9546, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9214, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0029, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9673, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7861, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9399, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7271, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9883, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7876, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0156, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.2432, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0723, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0693, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7822, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8213, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9854, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9033, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8447, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8818, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8403, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9292, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9209, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.7905, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9458, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8105, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.1152, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.9355, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.3652, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0020, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(0.8740, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n","tensor(1.0264, device='cuda:0', dtype=torch.float16, grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-c6843bd7560a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mxb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0myb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m    137\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}